# ai_engine.py
import requests

def call_gemma_chat(user_message):
    url = "http://192.168.168.58:1234/v1/chat/completions"
    headers = {"Content-Type": "application/json"}
    payload = {
        "model": "gemma-3-12b-it",
        "messages": [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä½é«˜ä¸­æ•¸å­¸è€å¸«ï¼Œè«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚"},
            {"role": "user", "content": user_message}
        ],
        "max_tokens": 2048,
        "temperature": 0.7
    }

    try:
        response = requests.post(url, headers=headers, json=payload)
        data = response.json()
        print("ğŸ§  Chat å›å‚³ JSONï¼š", data)  # â† ä¿ç•™è§€å¯Ÿæ ¼å¼

        # âœ… æ­£ç¢ºå–å¾— content
        return data["choices"][0]["message"]["content"].strip()
    except Exception as e:
        print("âŒ Chat æ¨¡å¼å¤±æ•—ï¼š", e)
        return "[éŒ¯èª¤] ç„¡æ³•å–å¾— Chat æ¨¡å‹å›æ‡‰"



def build_prompt(student_answer):
    return f"""
ä½ æ˜¯ä¸€ä½é«˜ä¸­æ•¸å­¸è€å¸«ï¼Œè«‹ä½ å¹«æˆ‘æ‰¹æ”¹ä»¥ä¸‹å­¸ç”Ÿä½œæ¥­å…§å®¹ï¼š

=== å­¸ç”Ÿä½œæ¥­é–‹å§‹ ===
{student_answer}
=== å­¸ç”Ÿä½œæ¥­çµæŸ ===

è«‹ä½ é‡å°ä»¥ä¸‹ä¸‰é»çµ¦å‡ºè©•èªèˆ‡å»ºè­°ï¼š
1. æ¦‚å¿µç†è§£ç¨‹åº¦
2. è¨ˆç®—éç¨‹æ­£ç¢ºæ€§
3. éŒ¯èª¤ä¿®æ­£å»ºè­°èˆ‡æœªä¾†å­¸ç¿’æ–¹å‘
è«‹ç”¨æ¢åˆ—æ–¹å¼å›è¦†ï¼Œä¸¦ä»¥ç¹é«”ä¸­æ–‡å›ç­”ã€‚
"""
